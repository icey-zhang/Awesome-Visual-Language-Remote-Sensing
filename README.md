# Awesome Awesome-Visual-Language-Remote-Sensing
<div align='center'>
  <img src=https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg >
  <img src=https://img.shields.io/github/downloads/DefTruth/Awesome-LLM-Inference/total?color=ccf&label=downloads&logo=github&logoColor=lightgrey >
  <img src=https://img.shields.io/github/forks/icey-zhang/Awesome-Visual-Language-Remote-Sensing.svg?style=social >
  <img src=https://img.shields.io/github/stars/icey-zhang/Awesome-Visual-Language-Remote-Sensing?style=social >
  <img src=https://img.shields.io/github/watchers/icey-zhang/Awesome-Visual-Language-Remote-Sensing?style=social >
  <img src=https://img.shields.io/badge/Release-v1.1-brightgreen.svg >
  <img src=https://img.shields.io/badge/License-GPLv3.0-turquoise.svg >
 </div>   
A curated list of neural network pruning and related resources. Inspired by [awesome-deep-vision](https://github.com/kjw0612/awesome-deep-vision), [awesome-adversarial-machine-learning](https://github.com/yenchenlin/awesome-adversarial-machine-learning), [awesome-deep-learning-papers](https://github.com/terryum/awesome-deep-learning-papers), [Awesome-NAS](https://github.com/D-X-Y/Awesome-NAS) and [awesome-pruning](https://github.com/he-y/Awesome-Pruning).

Please feel free to [pull requests](https://github.com/icey-zhang/Awesome-Visual-Language-Remote-Sensing/pulls) or [open an issue](https://github.com/icey-zhang/Awesome-Visual-Language-Remote-Sensing/issues) to add papers.

## Table of Contents

- [Type of Visual-Language-Remote-Sensing](#Type-of-Visual-Language-Remote-Sensing)
- [2023 Venues](#2023)
- [2024 Venues](#2024)

### Type-of-Visual-Language-Remote-Sensing

| Type        | Visual Question Answering |                  |               | Other     |
|:----------- |:--------------:|:--------------:|:----------------:|:-----------:|
| Explanation |    VQA   |                |                  | other types |

### 2023
| Title                                                                                                                            | Venue | Type    | Code |
|:-------------------------------------------------------------------------------------------------------------------------------- |:-----:|:-------:|:----:|
| [Multistep Question-Driven Visual Question Answering for Remote Sensing](https://ieeexplore.ieee.org/document/10242124)          | TRGS  |   VQA   | [PyTorch](https://github.com/MeimeiZhang-data/MQVQA)(Releasing) |
| [CLIP-guided Source-free Object Detection in Aerial Images](https://arxiv.org/abs/2401.05168) |igarss在投| source-free 目标检测|暂未开源|

### 2024
| Title                                                                                                                            | Venue | Type    | Code |
|:-------------------------------------------------------------------------------------------------------------------------------- |:-----:|:-------:|:----:|
| [SpectralGPT: Spectral Remote Sensing Foundation Model](https://arxiv.org/abs/2311.07113)          | TPAMI  |   Foundation Model   | [PyTorch](https://github.com/danfenghong/IEEE_TPAMI_SpectralGPT)(Releasing) |
| [SkySense: A Multi-Modal Remote Sensing Foundation Model Towards Universal Interpretation for Earth Observation Imagery](https://arxiv.org/abs/2312.10115)          | CVPR2024  |  Foundation Model    | 暂未开源|
| [GeoChat: Grounded Large Vision-Language Model for Remote Sensing](https://arxiv.org/abs/2311.15826)          | CVPR2024  |  LLM   | [pyTorch](https://github.com/mbzuai-oryx/GeoChat) |
| [MTP: Advancing Remote Sensing Foundation Model via Multi-Task Pretraining](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10547536)          | JSTAR2024  |  Foundation Model   | [pyTorch](https://github.com/ViTAE-Transformer/MTP) |
| [SkyScript: A Large and Semantically Diverse Vision-Language Dataset for Remote Sensing](https://arxiv.org/abs/2312.12856)          | AAAI2024  |  Image-text Dataset  | [pyTorch](https://github.com/wangzhecheng/SkyScript) |
| SkySenseGPT: A Fine-Grained Instruction Tuning Dataset and Model for Remote Sensing Vision-Language Understanding      | -  |  Vision-Language Understanding  | [pyTorch](https://github.com/Luo-Z13/SkySenseGPT) |
| [S2MAE: A Spatial-Spectral Pretraining Foundation Model for Spectral Remote Sensing Data](https://openaccess.thecvf.com/content/CVPR2024/html/Li_S2MAE_A_Spatial-Spectral_Pretraining_Foundation_Model_for_Spectral_Remote_Sensing_CVPR_2024_paper.html)      | CVPR2024  |  Foundation Model  | 暂未开源 |




